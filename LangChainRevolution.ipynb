{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unlocking the Potential of Large Language Models with LangChain\n",
        "\n",
        "\n",
        "[LangChain](https://python.langchain.com/en/latest/getting_started/getting_started.html) is a widely used framework that enables users to easily develop applications and pipelines using Large Language Models (LLMs). With LangChain, you can create chatbots, Generative Question-Answering (GQA) systems, summarization tools, and much more.\n",
        "\n",
        "At the core of LangChain's design is the idea of \"chaining\" together different components to create more sophisticated LLM use-cases. These chains can comprise multiple components from various modules, including:\n",
        "\n",
        "  * Prompt Templates: These templates serve as pre-defined structures for different types of prompts such as chatbot-style interactions, ELI5 question-answering, and more.\n",
        "\n",
        "  * Large Language Models: LLMs such as GPT-3 and BLOOM are used in LangChain to generate responses or perform language-related tasks.\n",
        "\n",
        "  * Agents: Agents utilize LLMs to determine the appropriate actions to take, using tools such as web search or calculators to execute operations within a logical loop.\n",
        "\n",
        "  * Memory: LangChain provides both short-term and long-term memory to help LLMs retain information and improve their responses over time.\n",
        "\n",
        "With these modules, LangChain offers a comprehensive solution for developing advanced LLM-based applications that can enhance natural language processing in various fields."
      ],
      "metadata": {
        "id": "wlbz3_5-mllE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g12MudNgIVd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qU openai langchain huggingface_hub transformes datasets bs4 tiktoken pinecone-client[grpc]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI21 LLM for Text Generation and Analysis"
      ],
      "metadata": {
        "id": "mQHXSQ54mKen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzjiHjqZyDeF",
        "outputId": "46a053e3-649b-40d4-c01d-d02c5638cbd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "id": "Bc5-o6yqyQuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# select a LLM model\n",
        "repo_id = \"google/flan-t5-xl\"\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jMGET8_JmKEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKXdX6tqtiBA",
        "outputId": "48c768a7-c74e-4d7e-8371-969d2856e578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "green bay packers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (\"question\") that is mapped to the question we'd like to ask."
      ],
      "metadata": {
        "id": "4mLmirTXmW6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We8_09QWmJ_U",
        "outputId": "4a39c829-f7e7-4e84-ccf1-757e32fa636d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='184', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with Chains\n",
        "\n",
        "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
        "\n",
        "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain — a `PromptTemplate` — to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain — a LLM.\n",
        "\n",
        "We'll start by importing all the libraries that we'll be using in this example."
      ],
      "metadata": {
        "id": "tT6eLC0jDSG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import re\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "\n",
        "def count_tokens(chain, query):\n",
        "  \"\"\"\n",
        "  This function prints and returns the count of token that were used during the run\n",
        "  \"\"\"\n",
        "  with get_openai_callback() as cb:\n",
        "    result = chain.run(query)\n",
        "    print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "WRO_CeFt9wRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vnAm4guJEBw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Agents \n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "Agents in LangChain\n"
      ],
      "metadata": {
        "id": "Lx-h2vxzp5cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agents use tools to improve their Answer"
      ],
      "metadata": {
        "id": "jDmhESHdld-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculator"
      ],
      "metadata": {
        "id": "-Sa28iKTloYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Search"
      ],
      "metadata": {
        "id": "57nROBAVlr0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone"
      ],
      "metadata": {
        "id": "fcWKcTzmlwt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Code"
      ],
      "metadata": {
        "id": "6yf2O8cvmw70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OPENAI API"
      ],
      "metadata": {
        "id": "wlWvLXY6pUBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
      ],
      "metadata": {
        "id": "4w_A_G7PpWhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
        "\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2VjBS08pXNs",
        "outputId": "1c7c0418-374c-48b1-a843-0b8777f689f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "BrightToes Socks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "print(question)\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj7XAzJpwSo6",
        "outputId": "c226ce5c-dda9-47e9-a2dd-0beacfa47b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which NFL team won the Super Bowl in the 2010 season?\n",
            " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "llm_chain.generate(qs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZtHBp1Q50Ab",
        "outputId": "c5a9dc0b-f82e-41ad-9f8a-f2faa36fdb8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 6 ft 4 inches is approximately 193.04 centimeters.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=' 193.04 centimeters', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The 12th person to walk on the moon was astronaut Alan L. Bean, who was part of the Apollo 12 mission in November 1969.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=' Eugene Cernan was the 12th person on the moon. He was part of the Apollo 17 mission in 1972, and was the last person to walk on the moon.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 199, 'completion_tokens': 124, 'prompt_tokens': 75}, 'model_name': 'text-davinci-003'})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
        "    \"Who was the 12th person on the moon?\",\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRB3x84m6QQq",
        "outputId": "af425493-c9e0-4260-80b4-cc2231a79ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. The New Orleans Saints\n",
            "2. 193.04 centimeters\n",
            "3. Harrison Schmitt\n",
            "4. None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoUxMNFy6XVn",
        "outputId": "b9ba8c5c-a048-4de9-f413-5ae0d63f65f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
            "If you are 6 ft 4 inches, you are 193.04 centimeters tall.\n",
            "The 12th person on the moon was Harrison Schmitt.\n",
            "A blade of grass does not have eyes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure of a Prompt\n",
        "\n",
        "A prompt can consist of multiple components:\n",
        "\n",
        "* Instructions\n",
        "* External information or context\n",
        "* User input or query\n",
        "* Output indicator\n",
        "\n",
        "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
        "\n",
        "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
        "\n",
        "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
        "\n",
        "**User input or query** is typically a query directly input by the user of the system.\n",
        "\n",
        "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
        "\n",
        "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
      ],
      "metadata": {
        "id": "QVKsCzIq7fHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples: \n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "llm.temperature = 1.0 # increase creativity/randomness of output\n",
        "\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boz6w9YA65XO",
        "outputId": "7f2b47ee-1981-4bfc-e18b-08b441ba2e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 42 -- the answer to life, the universe, and everything.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create a example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples: \n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "PG4dqODo8-qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the meaning of life?\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6gAGsO9GYE",
        "outputId": "1e9f890d-6269-42fb-b779-976aa70fd894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(\n",
        "    few_shot_prompt_template.format(query=query)\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJRiEy0S9qpy",
        "outputId": "315e310e-56c6-48db-cfb6-ef6608695a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To live life to the fullest and to find joy in everything you do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzeA3Yr5qcoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatting with ArXiv Research papers"
      ],
      "metadata": {
        "id": "62jpyjTqnCIs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MZOhaSfnKZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using other models in HuggingFace"
      ],
      "metadata": {
        "id": "aC3kcLp_nN0A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k-JygSxQncxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}